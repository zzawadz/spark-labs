{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/18 18:40:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# Create a SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyPySparkApp\") \\\n",
    "    .master(\"local[3]\") \\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+\n",
      "|shard|letter| id|\n",
      "+-----+------+---+\n",
      "|    0|     a|  0|\n",
      "|    1|     b|  1|\n",
      "|    2|     c|  2|\n",
      "|    0|     d|  3|\n",
      "|    1|     e|  4|\n",
      "|    2|     f|  5|\n",
      "+-----+------+---+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Create simple dataframe\n",
    "import string\n",
    "string.ascii_letters\n",
    "df = spark.createDataFrame([(i % 3, string.ascii_letters[i], i) for i in range(0, 20)], ['shard', 'letter', 'id'])\n",
    "df.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a partition?\n",
    "Each dataframe is split into partitions. Each partition allows for independent processing if no grouping is required. So when you call `df.withColumn('new_column', do_something)`, the execution might look like this:\n",
    "\n",
    "```\n",
    "Partition 0 - Calculated on first core:\n",
    "+-----+------+---+\n",
    "|shard|letter| id|\n",
    "+-----+------+---+\n",
    "|    0|     a|  0|\n",
    "|    1|     b|  1|\n",
    "|    2|     c|  2|\n",
    "+-----+------+---+\n",
    "Partition 1 Calculated on the second core:\n",
    "+-----+------+---+\n",
    "|shard|letter| id|\n",
    "+-----+------+---+\n",
    "|    0|     d|  3|\n",
    "|    1|     e|  4|\n",
    "|    2|     f|  5|\n",
    "+-----+------+---+\n",
    "...\n",
    "```\n",
    "### How many partitions should you have?\n",
    "\n",
    "It depends. In general, one partition should be about 100-200MB to keep the processor busy when executing. Also, there should be not to many of them, to avoid scheduling overhead.\n",
    "Also, a rule of thumb is that there should be 2-4x more partitions than execution cores. Otherwise we are wasting resources.\n",
    "\n",
    "How many partitions do we have here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6, 6, 8]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many partitions are used in that dataframe?\n",
    "print(f'Number of partitions: {df.rdd.getNumPartitions()}')\n",
    "\n",
    "# Old way of checking how many records are in each rdd?\n",
    "df.rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0|    6|\n",
      "|          1|    6|\n",
      "|          2|    8|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions  import spark_partition_id\n",
    "# spark_partition_id() gives the information on each partition a row is located\n",
    "df.withColumn(\"partitionId\", spark_partition_id()).groupBy(\"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+\n",
      "|partitionId|shard|count|\n",
      "+-----------+-----+-----+\n",
      "|          1|    0|    7|\n",
      "|          2|    1|    7|\n",
      "|          2|    2|    6|\n",
      "+-----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can repartition data so all the records with the same value will be located on the same partition\n",
    "df.repartition(3, 'shard').withColumn(\"partitionId\", spark_partition_id()).groupBy(\"partitionId\", \"shard\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(3, 'shard').rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bigger data frame for even more interesting experiments\n",
    "df_large = spark.createDataFrame(\n",
    "    [(i % 5, string.ascii_letters[i %24], i) for i in list(range(0, 200)) + [20] * 1000 ], ['shard', 'letter', 'id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----+\n",
      "|shard|partitionId|count|\n",
      "+-----+-----------+-----+\n",
      "|    3|          0|   40|\n",
      "|    2|          0|   40|\n",
      "|    0|          0|  240|\n",
      "|    1|          0|   40|\n",
      "|    4|          0|   40|\n",
      "|    0|          1|  400|\n",
      "|    0|          2|  400|\n",
      "+-----+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How the data was initially distributed:\n",
    "df_large.withColumn(\"partitionId\", spark_partition_id()).groupBy(\"shard\", \"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|shard|count|\n",
      "+-----+-----+\n",
      "|    0| 1040|\n",
      "|    1|   40|\n",
      "|    3|   40|\n",
      "|    2|   40|\n",
      "|    4|   40|\n",
      "+-----+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0|  400|\n",
      "|          1|  400|\n",
      "|          2|  400|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0|   40|\n",
      "|          5| 1040|\n",
      "|          7|   40|\n",
      "|          8|   40|\n",
      "|          9|   40|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_large.groupBy('shard').count().show()\n",
    "df_large.withColumn(\"partitionId\", spark_partition_id()).groupBy(\"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+\n",
      "|partitionId|shard|count|\n",
      "+-----------+-----+-----+\n",
      "|          0|    4|   40|\n",
      "|          5|    0| 1040|\n",
      "|          7|    3|   40|\n",
      "|          8|    2|   40|\n",
      "|          9|    1|   40|\n",
      "+-----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_large.repartition(10, 'shard').withColumn(\"partitionId\", spark_partition_id()).groupBy(\"partitionId\", \"shard\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can se above - all data associated with shard `0` was placed on one partition. Because of that we have a huge imbalance, which is called a skewed data. It's not good, because when doing processing, most probably other partitions will be ready way sooner that this one partition. Usually we try to avoid such cases. But this is a topic for another presentation. Just be careful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what is RDD?\n",
    "\n",
    "It's a data structure used by spark to allow recovering from executor failure. In fact, because of RDD Spark is lazy. The operations are not executed unitl the very last moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code does no spark processing\n",
    "import string\n",
    "string.ascii_letters\n",
    "df = spark.createDataFrame([(i % 3, string.ascii_letters[i], i) for i in range(0, 9)], ['shard', 'letter', 'id'])\n",
    "df = df.withColumn(\"partitionId\", spark_partition_id())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+-----------+\n",
      "|shard|letter| id|partitionId|\n",
      "+-----+------+---+-----------+\n",
      "|    0|     a|  0|          0|\n",
      "|    1|     b|  1|          0|\n",
      "|    2|     c|  2|          0|\n",
      "|    0|     d|  3|          1|\n",
      "|    1|     e|  4|          1|\n",
      "|    2|     f|  5|          1|\n",
      "|    0|     g|  6|          2|\n",
      "|    1|     h|  7|          2|\n",
      "|    2|     i|  8|          2|\n",
      "+-----+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 19:11:24,202 - SPARK - INFO - d - partition_id: 1        (0 + 3) / 3]\n",
      "2025-01-18 19:11:24,208 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:11:24,224 - SPARK - INFO - g - partition_id: 2\n",
      "2025-01-18 19:11:24,703 - SPARK - INFO - e - partition_id: 1\n",
      "2025-01-18 19:11:24,708 - SPARK - INFO - b - partition_id: 0\n",
      "2025-01-18 19:11:24,725 - SPARK - INFO - h - partition_id: 2\n",
      "2025-01-18 19:11:25,204 - SPARK - INFO - f - partition_id: 1\n",
      "2025-01-18 19:11:25,209 - SPARK - INFO - c - partition_id: 0\n",
      "2025-01-18 19:11:25,225 - SPARK - INFO - i - partition_id: 2\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One partition:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 19:11:27,883 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:11:28,384 - SPARK - INFO - b - partition_id: 0\n",
      "2025-01-18 19:11:28,884 - SPARK - INFO - c - partition_id: 0        (0 + 1) / 1]\n",
      "2025-01-18 19:11:29,385 - SPARK - INFO - d - partition_id: 0\n",
      "2025-01-18 19:11:29,886 - SPARK - INFO - e - partition_id: 0\n",
      "2025-01-18 19:11:30,386 - SPARK - INFO - f - partition_id: 0\n",
      "2025-01-18 19:11:30,887 - SPARK - INFO - g - partition_id: 0\n",
      "2025-01-18 19:11:31,387 - SPARK - INFO - h - partition_id: 0\n",
      "2025-01-18 19:11:31,888 - SPARK - INFO - i - partition_id: 0\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two partition:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 19:11:34,559 - SPARK - INFO - b - partition_id: 1\n",
      "2025-01-18 19:11:34,561 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:11:35,060 - SPARK - INFO - c - partition_id: 1\n",
      "2025-01-18 19:11:35,061 - SPARK - INFO - f - partition_id: 0\n",
      "2025-01-18 19:11:35,561 - SPARK - INFO - e - partition_id: 1        (0 + 2) / 2]\n",
      "2025-01-18 19:11:35,562 - SPARK - INFO - i - partition_id: 0\n",
      "2025-01-18 19:11:36,062 - SPARK - INFO - d - partition_id: 1\n",
      "2025-01-18 19:11:36,062 - SPARK - INFO - g - partition_id: 0\n",
      "2025-01-18 19:11:36,562 - SPARK - INFO - h - partition_id: 1\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "from time import sleep\n",
    "import logging\n",
    "\n",
    "def string_length(name, partitionId):\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,  # Set the logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",  # Define the log format\n",
    "        handlers=[\n",
    "            logging.StreamHandler()  # Output logs to the console\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger('SPARK')\n",
    "    logger.info(f'{name} - partition_id: {partitionId}')\n",
    "    sleep(0.5)\n",
    "    return len(name) if name else 0\n",
    "string_length_udf = udf(string_length, IntegerType())\n",
    "\n",
    "x = df.withColumn(\"name_length\", string_length_udf(df[\"letter\"], df['partitionId'])).collect()\n",
    "sleep(2)\n",
    "print(\"One partition:\")\n",
    "x = df.repartition(1).withColumn(\"partitionId\", spark_partition_id()).withColumn(\"name_length\", string_length_udf(df[\"letter\"], col('partitionId'))).collect()\n",
    "sleep(2)\n",
    "print(\"Two partition:\")\n",
    "x = df.repartition(2).withColumn(\"partitionId\", spark_partition_id()).withColumn(\"name_length\", string_length_udf(df[\"letter\"], col('partitionId'))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 19:13:05,575 - SPARK - INFO - c - partition_id: 3\n",
      "2025-01-18 19:13:05,576 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:13:05,615 - SPARK - INFO - b - partition_id: 4\n",
      "2025-01-18 19:13:06,076 - SPARK - INFO - f - partition_id: 3\n",
      "2025-01-18 19:13:06,076 - SPARK - INFO - d - partition_id: 0\n",
      "2025-01-18 19:13:06,116 - SPARK - INFO - e - partition_id: 4\n",
      "2025-01-18 19:13:06,577 - SPARK - INFO - i - partition_id: 3        (0 + 3) / 5]\n",
      "2025-01-18 19:13:06,577 - SPARK - INFO - g - partition_id: 0\n",
      "2025-01-18 19:13:06,616 - SPARK - INFO - h - partition_id: 4\n",
      "2025-01-18 19:13:07,078 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:13:07,578 - SPARK - INFO - a - partition_id: 0        (4 + 1) / 5]\n",
      "2025-01-18 19:13:08,079 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:13:08,579 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:13:09,080 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:13:09,581 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:13:10,081 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:13:10,582 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:13:11,083 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:13:11,583 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:13:12,084 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:13:12,585 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:13:13,085 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:13:13,586 - SPARK - INFO - a - partition_id: 0\n",
      "2025-01-18 19:13:14,087 - SPARK - INFO - a - partition_id: 0\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Skew data in action\n",
    "df = spark.createDataFrame([(i % 3, string.ascii_letters[i], i) for i in list(range(0, 9)) + [0] * 15], ['shard', 'letter', 'id'])\n",
    "df = df.repartition(5,'shard').withColumn(\"partitionId\", spark_partition_id())\n",
    "x = df.withColumn(\"name_length\", string_length_udf(df[\"letter\"], df['partitionId'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(i % 3, string.ascii_letters[i], i) for i in range(0, 9)], ['shard', 'letter', 'id'])\n",
    "df = df.withColumn(\"partitionId\", spark_partition_id())\n",
    "df = df.withColumn(\"name_length\", string_length_udf(df[\"letter\"], df['partitionId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame([(string.ascii_letters[i], i *100) for i in range(0, 5)], ['letter', 'id100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.join(df2, how='inner', on='letter').show()\n",
    "df2.groupBy(['letter']).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(['letter']).count().show()\n",
    "df.groupBy(['letter','name_length']).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()\n",
    "df.join(df2, how='inner', on='letter').show()\n",
    "df.groupBy(['letter','name_length']).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.join(df2, how='inner', on='letter').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(['letter','name_length']).count().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(3, 'shard').write.partitionBy('shard').mode('overwrite').parquet('tables/df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = spark.read.parquet('tables/df').withColumn(\"partitionId\", spark_partition_id()).withColumn(\"name_length\", string_length_udf(col(\"letter\"), col('partitionId'))).collect()\n",
    "sleep(2)\n",
    "print(\"Filter:\")\n",
    "x = spark.read.parquet('tables/df').filter(col('shard') == 1).withColumn(\"partitionId\", spark_partition_id()).withColumn(\"name_length\", string_length_udf(col(\"letter\"), col('partitionId'))).collect()\n",
    "\n",
    "print(\"Filter after:\")\n",
    "x = spark.read.parquet('tables/df').withColumn(\"partitionId\", spark_partition_id()).withColumn(\"name_length\", string_length_udf(col(\"letter\"), col('partitionId'))).filter(col('shard') == 1).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet('tables/df').filter(col('shard') == 1).withColumn(\"partitionId\", spark_partition_id()).withColumn(\"name_length\", string_length_udf(col(\"letter\"), col('partitionId'))).explain(mode=\"extended\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet('tables/df').withColumn(\"partitionId\", spark_partition_id()).withColumn(\"name_length\", string_length_udf(col(\"letter\"), col('partitionId'))).filter(col('shard') == 1).explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
